<!DOCTYPE html>
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width"><link rel="stylesheet" type="text/css" media="all" href="../css/article_styles.css"><script type='text/x-mathjax-config'>  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script><script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script></head>
<body><div class='content'><h1 id="word2vec 笔记">word2vec 笔记</h1>

<h2 id="目标">目标</h2>
将一段文本中的一个词转化为一个向量，对于不同的词可以通过其向量之间的点积标识其语义相近程度
<h2 id="主要基本步骤">主要基本步骤</h2><div class="liRank0"><li class="liRank0"><strong>读入一段文本，通过一段固定的“窗口”提取样本</strong></li></div>
<div class="image"><image src="assets/2018-2-28_TF_Note_3_word2vec-5528b.png" title=""></image></div>
<div class="liRank0"><li class="liRank0"><strong>将每一个单词标识为一个one hot向量(例子为10000)，而设置一定的学习神经元（例子为300），对应建立分类器（与单词数目相同：10000）构成如下结构</strong></li></div>
<div class="image"><image src="assets/2018-2-28_TF_Note_3_word2vec-4710f.png" title=""></image></div>
<div class="liRank0"><li class="liRank0"><strong>对于一个需要查询的单词，首先是使用其对应的one hot向量去乘以输入权重矩阵（从input vector到各个hidden layer neutron）得到中间向量</strong></li></div>
&emsp;在例子里面：10000个单词，300个特征维度（中间神经元数目），则
&emsp;&emsp;input vector为  $w_i=[0,0,...,1,0,...,0]_{1\times 10000} $
&emsp;&emsp;weight maxtrix为  $W_{10000\times 300} $
&emsp;&emsp;产生的中间向量为  $[a_{i1},a_{i2},...,a_{i300}]_{1\times 300} $
<strong>对于中间向量，再经过输出层的分类（矩阵运算后进行softmax），得到一个最终的概率分布向量</strong>
&emsp;&emsp;输出层矩阵 $W'_{300\times 10000}$
&emsp;&emsp;产生的最终向量为  $1\times 10000 $ 的向量
<div class="image"><image src="assets/2018-2-28_TF_Note_3_word2vec-968d8.png" title=""></image></div>
上图划线表示了不同小逻辑单位（每个neutron或者softmax classifier）
<div class="image"><image src="assets/2018-2-28_TF_Note_3_word2vec-5aeb9.png" title="原理示意图"></image></div>

<h2 id="Problems and Solution">Problems and Solution</h2>
<h3 id="问题">问题</h3><div class="ulRank0"><ul class="ulRank0">1. 大样本下原有的梯度计算方法沉重（两个权重矩阵，均为$|v|	imes|d|$大小，其中|v|为vocabulary大小,|d|为neutron number 也是feature dimension）</ul></div><div class="ulRank0"><ul class="ulRank0">2. softmax非常沉重</ul></div>
<h3 id="思路">思路</h3><div class="ulRank0"><ul class="ulRank0">1. 常见词组合并为短语(phrase)</ul></div><div class="ulRank0"><ul class="ulRank0">2. 在原本样本集合中再增加一层采样(subsampling)</ul></div><div class="ulRank0"><ul class="ulRank0">3. Sample-based approach vs Softmax</ul></div>
&emsp;使用nagative sampling\NCE方法，只对少量权重进行反馈修整(nagative sampling)
<h3 id="合并词组">合并词组</h3><div class="liRank0"><li class="liRank0">每次合并仅仅两个词，但是可以多次合并，使得合并后的phrase与新的词或者phrase合并成为更长的phrase</li></div><div class="liRank0"><li class="liRank0">记录词的组合出现的次数，利用方程比较，将一起出现多于单独出现的词合并</li></div><div class="liRank0"><li class="liRank0">优先将不经常出现的词组合并，避免常用句式（this is等）影响</li></div>
<h3 id="再采样">再采样</h3><div class="ulRank0"><ul class="ulRank0">1. 无意义组合删除，如('the'，'xxx')等</ul></div><div class="ulRank0"><ul class="ulRank0">2. 采样率修正：对于不常出现的词，删去（具体操作时采用替换为特殊已知词组，再主动忽略）</ul></div>
单词保留概率

\[
z(w_i)=\frac{N_{word}}{N_{corpus}}
\]

\[
P(w_i)=(\sqrt{\frac{z(w_i)}{0.001}}+1)\frac{0.001}{z(w_i)}
\]


<div class="image"><image src="assets/2018-2-28_TF_Note_3_word2vec-fcd45.png" title=""></image></div>

$y=\frac{sqrt(x)+1}{x}$

<h3 id="negative sampling（Sample-based approach）">negative sampling（Sample-based approach）</h3>
<h4 id="作用">作用</h4>
避免计算所有权重（全部更新：一次样本输入，需要更新10,000x300的权重）
<h4 id="做法 ">做法 </h4><div class="liRank0"><li class="liRank0">对于一个样本（A,B）,将A作为中心词输入，计算，理想输出应该是关于B的one hot vector，通常做法是全部softmax直接计算反馈更新</li></div><div class="liRank0"><li class="liRank0">nagative sampling:更新时，对于反馈更新：</li></div>
&emsp;<strong>1.</strong> 更新classifier matrix的权重仅仅更新一个目标正确判断的classifier（B对应的classifier）（<strong>positive sample</strong>）和随机选取的若干个（如5个）错误判断（理想vector为0的位置）(<strong>nagative sample</strong>)的classifier的权重（更新6x300）
&emsp;<strong>2.</strong> 更新hidden layer matrix的权重，仅仅更新与输入向量（对应A）有关的权重（更新1x300）__（这一步与Nagative Sampling无关）__

<h4 id="nagative sample 的选取法则">nagative sample 的选取法则</h4>
选取概率
\[
P(w_i)=\frac{freq(w_i)^\frac{3}{4}}{\Sigma{freq(w)^\frac{3}{4}}}
\]
<strong>$rac{3}{4}$为magic number</strong>
<h3 id="NCE:Noise Contrastive Estimation（Sample-based approach）">NCE:Noise Contrastive Estimation（Sample-based approach）</h3>
<h4 id="变量约定">变量约定</h4>
<table cellspacing='0'><tr><th> variable             </th><th> meaning                                             </th></tr><tr><td> $	heta$             </td><td> parameter                                           </td></tr><tr><td> $w$                  </td><td> word                                                </td></tr><tr><td> $c$                  </td><td> context                                             </td></tr><tr><td> $s_	heta(w_i,c)$    </td><td> score                                               </td></tr><tr><td> $u_	heta(w_i,c)$    </td><td> $e^{s_	heta(w_i,c)}$                               </td></tr><tr><td> $Z(c)$               </td><td> $\Sigma_{w}(u_	heta,c)$                            </td></tr><tr><td> $\widetilde{p} (w_i\</td><td> c)$ or $\widetilde{p}(c)$ </td><td> empirical distributions </td></tr><tr><td> $q(w)$               </td><td> noise distribution                                  </td></tr></table><br>
\[
p_\theta(w_i|c)=\frac{u_\theta(w_i,c)}{\Sigma_{w}(u_\theta,c)}
=\frac{u_\theta(w_i,c)}{Z_\theta(c)}
\]
<h2 id="参考资料">参考资料</h2>
Chris McCormick：
<a class="url" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a>
<a class="url" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">Word2Vec Tutorial Part 2 - Negative Sampling</a>
Chris Dyer :
<a class="url" href="https://arxiv.org/pdf/1410.8251.pdf">Notes on Noise Contrastive Estimation and Negative Sampling</a></div></body>
</html>
