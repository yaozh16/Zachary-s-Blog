<!DOCTYPE html>
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width"><link rel="stylesheet" type="text/css" media="all" href="../css/article_styles.css"><script type='text/x-mathjax-config'>  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script><script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script></head>
<body><div class='content'><h1 id="TF Read Data">TF Read Data</h1>
<h2 id="主要方法">主要方法</h2><div class="ulRank0"><ul class="ulRank0">1. Feeding：代码送入 ```Python code provides the data when running each step.```</ul></div><div class="ulRank0"><ul class="ulRank0">2. Reading from files：管道送入 ```an input pipeline reads the data from files at the beginning of a TensorFlow graph.```</ul></div><div class="ulRank0"><ul class="ulRank0">3. Preloaded data: ```a constant or variable in the TensorFlow graph holds all the data (for small data sets).```</ul></div>
<h2 id="Feeding">Feeding</h2><div class='citation'><p class='citationP'><br>Supply feed data through the feed_dict argument to a run() or eval() call that initiates computation.
</p></div>
<div class='codeBlock'><pre class='codeBlockPre'><blockquote>with tf.Session():
  input = tf.placeholder(tf.float32)
  classifier = ...
  print(classifier.eval(feed_dict={input: my_python_preprocessing_fn()}))
</blockquote></pre></div>

An example using placeholder and feeding to train on MNIST data can be found in <a class="url" href="https://raw.githubusercontent.com/tensorflow/tensorflow/r0.12/tensorflow/examples/tutorials/mnist/fully_connected_feed.py">tensorflow/examples/tutorials/mnist/fully_connected_feed.py</a>, and is described in the MNIST tutorial
<h2 id="Reading from files">Reading from files</h2><div class="liRank0"><li class="liRank0">The list of filenames</li></div><div class="liRank0"><li class="liRank0">Optional filename shuffling</li></div><div class="liRank0"><li class="liRank0">Optional epoch limit</li></div><div class="liRank0"><li class="liRank0">Filename queue</li></div><div class="liRank0"><li class="liRank0">A Reader for the file format</li></div><div class="liRank0"><li class="liRank0">A decoder for a record read by the reader</li></div><div class="liRank0"><li class="liRank0">Optional preprocessing</li></div><div class="liRank0"><li class="liRank0">Example queue</li></div>
<h3 id="Filenames, shuffling, and epoch limits">Filenames, shuffling, and epoch limits</h3>
<h4 id="the list of filenames">the list of filenames</h4><div class="ulRank0"><ul class="ulRank0">1. a constant string Tensor (like ["file0", "file1"] or [("file%d" % i) for i in range(2)])</ul></div><div class="ulRank0"><ul class="ulRank0">2. the ```tf.train.match_filenames_once``` function</ul></div>
<h3 id="File formats">File formats</h3>
choose decoder to transform the data into tensors
<h4 id="CSV(comma-separated value) files">CSV(comma-separated value) files</h4>
for examle:
<div class='codeBlock'><pre class='codeBlockPre'><blockquote><h1 id="read data">read data</h1>
filename_queue = tf.train.string_input_producer([&quot;file0.csv&quot;, &quot;file1.csv&quot;])

reader = tf.TextLineReader()
key, value = reader.read(filename_queue)

<h1 id="Default values, in case of empty columns. Also specifies the type of the">Default values, in case of empty columns. Also specifies the type of the</h1>
<h1 id="decoded result.">decoded result.</h1>
record_defaults = [[1], [1], [1], [1], [1]]
col1, col2, col3, col4, col5 = tf.decode_csv(
    value, record_defaults=record_defaults)
features = tf.pack([col1, col2, col3, col4])

with tf.Session() as sess:
  <h1 id="Start populating the filename queue.">Start populating the filename queue.</h1>
  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(coord=coord)<h1 id="需要先执行这一步才能防止阻塞">需要先执行这一步才能防止阻塞</h1>

  for i in range(1200):
    <h1 id="Retrieve a single instance:">Retrieve a single instance:</h1>
    example, label = sess.run([features, col5])

  coord.request_stop()
  coord.join(threads)

</blockquote></pre></div>
<h4 id="Fixed length records">Fixed length records</h4><div class='citation'><p class='citationP'><br>To read binary files in which each record is a fixed number of bytes, use ```tf.FixedLengthRecordReader``` with the ```tf.decode_raw``` operation. The decode_raw op converts from a string to a uint8 tensor.
</p></div>
the CIFAR-10 dataset uses a file format where each record is represented using a fixed number of bytes: 1 byte for the label followed by 3072 bytes of image data.Once you have a uint8 tensor, standard operations can slice out each piece and reformat as needed. For CIFAR-10, you can see how to do the reading and decoding in <a class="url" href="https://raw.githubusercontent.com/tensorflow/tensorflow/r0.12/tensorflow/models/image/cifar10/cifar10_input.py">tensorflow/models/image/cifar10/cifar10_input.py</a> and described in <a class="url" href="https://www.tensorflow.org/versions/r0.12/tutorials/deep_cnn/index.html#prepare-the-data">this tutorial</a>.
<h4 id="Standard TensorFlow format">Standard TensorFlow format</h4><div class='citation'><p class='citationP'><br>Another approach is to convert whatever data you have into a supported format. This approach makes it easier to mix and match data sets and network architectures. The recommended format for TensorFlow is a ```TFRecords``` file containing ```tf.train.Example``` protocol buffers (which contain ```Features``` as a field). You write a little program that gets your data, stuffs it in an Example protocol buffer, serializes the protocol buffer to a string, and then writes the string to a TFRecords file using the tf.python_io.TFRecordWriter class. For example, <a class="url" href="https://raw.githubusercontent.com/tensorflow/tensorflow/r0.12/tensorflow/examples/how_tos/reading_data/convert_to_records.py">tensorflow/examples/how_tos/reading_data/convert_to_records.py</a> converts MNIST data to this format.
</p></div><h3 id="Preprocessing">Preprocessing</h3><div class="liRank0"><li class="liRank0">This would be any processing that <strong>doesn't</strong> depend on trainable parameters</li></div>

example at <a class="url" href="https://www.tensorflow.org/code/tensorflow/models/image/cifar10/cifar10_input.py">tensorflow/models/image/cifar10/cifar10_input.py</a>
<h3 id="Batching">Batching</h3>
At the end of the pipeline we use another queue to batch together examples for training, evaluation, or inference. For this we use a queue that randomizes the order of examples, using the ```tf.train.shuffle_batch``` function.
<table cellspacing='0'><tr><th> variable          </th><th> function                                                                                                                         </th></tr><tr><td> min_after_dequeue </td><td> defines how big a buffer we will randomly sample from -- bigger means better shuffling but slower start up and more memory used. </td></tr><tr><td> capacity          </td><td> must be larger than ```min_after_dequeue``` and the amount larger  determines the maximum we will prefetch.                      </td></tr></table><br>
Recommendation: 
\[
capacity=min_{}After_{}Dequeue + (num_{}Threads + a Small Safety Margin) * batch_{}Size
\]
<div class='codeBlock'><pre class='codeBlockPre'><blockquote>def read_my_file_format(filename_queue):
  reader = tf.SomeReader()
  key, record_string = reader.read(filename_queue)
  example, label = tf.some_decoder(record_string)
  processed_example = some_processing(example)
  return processed_example, label

def input_pipeline(filenames, batch_size, num_epochs=None):
  filename_queue = tf.train.string_input_producer(
      filenames, num_epochs=num_epochs, shuffle=True)
  example, label = read_my_file_format(filename_queue)
  min_after_dequeue = 10000
  capacity = min_after_dequeue + 3 * batch_size
  example_batch, label_batch = tf.train.shuffle_batch(
      [example, label], batch_size=batch_size, capacity=capacity,
      min_after_dequeue=min_after_dequeue)
  return example_batch, label_batch
</blockquote></pre></div><div class='citation'><p class='citationP'><br>if you need more parallelism or shuffling of examples between files, use multiple reader instances using the ```tf.train.shuffle_batch_join``` function
</p></div>
<div class='codeBlock'><pre class='codeBlockPre'><blockquote>def input_pipeline(filenames, batch_size, read_threads, num_epochs=None):
  filename_queue = tf.train.string_input_producer(
      filenames, num_epochs=num_epochs, shuffle=True)
  example_list = [read_my_file_format(filename_queue)
                  for _ in range(read_threads)]
  min_after_dequeue = 10000
  capacity = min_after_dequeue + 3 * batch_size
  example_batch, label_batch = tf.train.shuffle_batch_join(
      example_list, batch_size=batch_size, capacity=capacity,
      min_after_dequeue=min_after_dequeue)
  return example_batch, label_batch
</blockquote></pre></div>
###Creating threads to prefetch using QueueRunner objects
启动读取，否则永远阻塞等待数据<div class='citation'><p class='citationP'><br>call ```tf.train.start_queue_runners``` before running any training or inference steps, or it will hang forever. This will start threads that run the input pipeline, filling the example queue so that the dequeue to get the examples will succeed. This is best combined with a ```tf.train.Coordinator``` to cleanly shut down these threads when there are errors. If you set a limit on the number of epochs, that will use an epoch counter that will need to be initialized. 
</p></div>
Example:
<div class='codeBlock'><pre class='codeBlockPre'><blockquote><h1 id="Create the graph, etc.">Create the graph, etc.</h1>
init_op = tf.global_variables_initializer()

<h1 id="Create a session for running operations in the Graph.">Create a session for running operations in the Graph.</h1>
sess = tf.Session()

<h1 id="Initialize the variables (like the epoch counter).">Initialize the variables (like the epoch counter).</h1>
sess.run(init_op)

<h1 id="Start input enqueue threads.">Start input enqueue threads.</h1>
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)

try:
    while not coord.should_stop():
        <h1 id="Run training steps or whatever">Run training steps or whatever</h1>
        sess.run(train_op)

except tf.errors.OutOfRangeError:
    print(&apos;Done training -- epoch limit reached&apos;)
finally:
    <h1 id="When done, ask the threads to stop.">When done, ask the threads to stop.</h1>
    coord.request_stop()

<h1 id="Wait for threads to finish.">Wait for threads to finish.</h1>
coord.join(threads)
sess.close()
</blockquote></pre></div>
<div class="image"><image src="assets/AnimatedFileQueues.gif" title=""></image></div></div></body>
</html>
